{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8647f57-c8ad-45b8-91e6-b3549d6d3317",
   "metadata": {},
   "source": [
    "<div class=\"alert\">  \n",
    "    <center><h1><strong>Final Project: NLP Fake news detection</strong></h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addb5e8e-004d-4b16-a215-9610bf022554",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">  \n",
    "    <center><h2><strong>Analysis / exploration of the data set</strong></h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2566cf-fc40-4b52-a87e-08f68b7cd53c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b>Summary of the section </b> \n",
    "<hr>\n",
    "<ul>\n",
    "    <li>Analyze the chosen dataset and its properties</li>\n",
    "</ul>\n",
    "    \n",
    "<hr>\n",
    "<b>Source of Data: </b> \n",
    "<hr> \n",
    " <a href=\"https://www.cs.ucsb.edu/~william/data/liar_dataset\">https://www.cs.ucsb.edu/~william/data/liar_dataset</a>\n",
    "   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40bf932-87d7-4f15-a0ee-dd27269d6298",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf98781-ad09-4fa0-951d-b3f3bb54053b",
   "metadata": {},
   "source": [
    "## 1. Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0039f34b-1a8a-4199-a9e2-091fec0dab27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are loading th dataset into a pandas dataframe we can do further analysis on it\n",
    "df = pd.read_csv(\"datasets/Liar_Dataset.csv\")\n",
    "\n",
    "# We are also going to look at the first 5 records in it so to have an idead on what we are working with\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5fa2f7-1cd6-4c66-8ab6-5a3f8c02bb91",
   "metadata": {},
   "source": [
    "## 2. Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d647a4-7999-4f7c-93d6-785795f15f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us first take a look at what we are dealing with\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4b51bb-1e05-4d0e-8b36-17a8e300f1bc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b>In the above we can point a few things about the dataset</b> \n",
    "<hr>\n",
    "<ul>\n",
    "    <li>1. It has 14 columns and 12787 rows</li>\n",
    "    <li>2. It has two datatypes: objects (strings) and int64 (numbers) </li>\n",
    "    <li>3. The first column - which seems to be some kind of ID - is actually a string. And we can see by looking at the data inside the CSV looks like the filename.</li>\n",
    "    <li>4. We can also notice here that we have a few nulls, as for some columns (such as) 'speaker's job title ', 'state info' and 'venue' the Non-Null Counts are lower than the number of rows. </li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032528dd-b220-4018-9a93-9caaafc667ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now take a look at the columns inside the dataset so to have an idea about the features we can use further in our model\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea34e9d-af2f-4d0a-88f5-4e593946850d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b>Further description by matching the columns with the rows</b> \n",
    "<hr>\n",
    "<ul>\n",
    "    <li>1. '[ID].json' - As already stated, this seems to be a filename. This may not be an useful feature for us to use so we may delete it in the future.</li>\n",
    "    <li>2. 'label' -  This column is the classification of the news statement, it can be either TRUE, FALSE, half-true, half-false or pants-fire.</li>\n",
    "    <li>3. 'statement' - This represents the text in the news article. </li>\n",
    "    <li>3. 'subject(s)' -   The subjects that the news statement talks about. We will further analyze what are them below. </li>\n",
    "    <li>3. 'speaker' -  The person who talked/wrote about the news statement.  We will further analyze what are them below. </li>\n",
    "    <li>4. 'speakers job title' -  The speaker's job.  We will further analyze what are them below. </li>\n",
    "    <li>5. 'state info' -  ???</li>\n",
    "    <li>7. 'party affiliation' - The political party the speaker is related to. </li>\n",
    "    <li>8. 'barely true counts' - The counts of statements that are classified as being barely true. Meaning ??? </li>\n",
    "    <li>9. 'false counts' - The counts of statements that are classified as being barely false </li>\n",
    "    <li>10. 'half true counts' -  The counts of statements that are classified as being half true. Meaning ??? </li>\n",
    "    <li>11. 'mostly true counts' -  The counts of statements that are classified as being mostly true. Meaning ??? </li>\n",
    "    <li>12. 'pants on fire counts' -  The counts of statements that are classified as being pants on fire. Meaning ??? </li>\n",
    "    <li>13. 'venue' - The \"place\" where the statement was said. It could be a radio station, a blog or any other media venue. </li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6afaa3-5692-4372-b1a8-be8d419977d5",
   "metadata": {},
   "source": [
    "## 3. Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35c77e4-cbdd-4121-9d6b-5f019b83967e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We already noticed above that we have some null values. Now let us confirm how many and in which columns\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223e4ea2-305d-488f-983f-da1009eb7d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(df.isnull().any(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94463b21-d94d-40b7-9ed6-a9cf9e1cb0a9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b>In the above we can see that:</b> \n",
    "<hr>\n",
    "<ul>\n",
    "    <li>1. We have 3565 nulls in the speaker's job title column, 2747 nulls in the state info column, and 129 nulls in the venue column.</li>\n",
    "    <li>2. In total we have 4351 nulls</li>\n",
    "    <li>3. We can also see by analysing the rows that we have two kids of nulls when the value is missing: NaN and tje np.nan return type.</li>\n",
    "</ul>\n",
    "<h3>Below we will apply some strategies to clean these nulls in order to not mess with our analysis.</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf16f49c-6aee-4fe4-a3f9-f8aa1b533369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are re replacing the rows with NaN (null) values with a blank space\n",
    "df.replace('', np.nan, inplace=True)\n",
    "\n",
    "# Now we are going to replace the np.nan values for all the three columns we are cleaning\n",
    "df[\"speaker's job title\"]= df[\"speaker's job title\"].replace(np.nan, 'Unknown')\n",
    "df['venue']= df['venue'].replace(np.nan, 'Unknown')\n",
    "df[\"state info\"]= df[\"state info\"].replace(np.nan, 'Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36c3301-7a59-495c-84ab-c8ccd50fc0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let us check again if we were able to completely remove the nulls\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd5191d-d7ec-46f5-8961-db2b4f25f489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also remove th [ID].json column, since it is not useful to us\n",
    "df.drop(columns=['[ID].json'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724071b5-e6af-412b-8925-f9f1f20fcfb7",
   "metadata": {},
   "source": [
    "## 4. Data visualization on the rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb93ecd-9776-40f8-8bbb-83dc6934f3c8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b>Below we are going to try to understand more about the rows we have so to start reasoning about which information we have in them and which of them we can use later to extract the features for our Machine Learning models.</b> \n",
    "<hr>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff208c4-b61f-4ce0-baea-d65f3fd90e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some helper functions and variables for generating the visualizations using matplotlib, wordcloud and seaborn\n",
    "from matplotlib import pyplot as plt \n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "\n",
    "top10 = 10\n",
    "\n",
    "\n",
    "# Plots a pie graph based on a specific column\n",
    "def plot_pie(column, number_of_values):\n",
    "    df[column].value_counts().head(number_of_values).plot(kind = 'pie', autopct='%1.1f%%', figsize=(8, 8)).legend(bbox_to_anchor=(1, 1))\n",
    "\n",
    "def plot_bar(column, number_of_values):\n",
    "    df[column].value_counts().value_counts().head(number_of_values).plot(x='lab', y='val', rot=0)\n",
    "\n",
    "# TODO: Fix because we need a way to show the data correlation between different columns.\n",
    "# Plots a wordcloud based on the relationship between two columns\n",
    "## column: \n",
    "## value:\n",
    "## word:\n",
    "# def plot_wordcloud(column, value, word):\n",
    "#     data1=df[df[column]==value]\n",
    "#     d =data1[word]\n",
    "#     string_ = []\n",
    "#     for t in d:\n",
    "#         string_.append(t)\n",
    "#     string_ = pd.Series(string_).map(str)\n",
    "#     string_=str(string_)\n",
    "#     wc = WordCloud(width=1500, height=700,max_font_size=250, background_color ='white').generate(string_)\n",
    "#     plt.figure(figsize=(12,10))\n",
    "#     plt.imshow(wc)\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbcef85-551f-4a20-aaee-f2fda7595313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a pie graph based on the label column\n",
    "plot_pie('label', top10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a786bb7-b24c-4c3e-9ba0-8373927cdeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a pie graph based on the subjects column\n",
    "plot_pie('subject(s)', top10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bbc9c2-b33e-41fc-bcee-6758fe81d61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a pie graph based on the subjects speakers\n",
    "plot_pie('speaker', top10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da63fe4-10a8-448a-b03d-db4ab0c740cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a pie graph based on the speaker's job title column\n",
    "plot_pie(\"speaker's job title\", top10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9cb91e-e10c-4f3f-aa43-97e7fe922f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a pie graph based on the venue\n",
    "plot_pie(\"venue\", top10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6364fef-420d-498c-8335-4538694b8b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a pie graph based on the speaker's job title column\n",
    "plot_pie(\"state info\", top10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39c0e51-8c86-4464-a61f-567d75a62433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a pie graph based on the speaker's job title column\n",
    "plot_pie(\"party affiliation\", top10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b7e788-49a1-40a5-ac61-b9f49070007a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b>Based on the above pie charts we can already draw some interesting conclusions:</b> \n",
    "<hr>\n",
    "<ul>\n",
    "<li>1. For the <b>'label'</b> column, which defines the category to which the news articles belong to, we can see that our dataset contains mostly half-true statements (19.6% of the overall). On the opposite, lower end we have 8.2% pants-fire statements. </li>\n",
    "<li>2. As for the subjects in the dataset, we see 10 of them, being (in order of distribution of statements, from higher to lower):\n",
    " a. Health-care\n",
    " b. education\n",
    " c. elections\n",
    " d. immigration\n",
    " c. candidates-biography\n",
    " d. economy </li>\n",
    "<li>3. As for the <b>job titles</b> of the speakers in the dataset (the person who wrote/spoke the news article) we have them being:\n",
    " a. Unknown - the articles we had no data for speaker\n",
    " b. President\n",
    " c. U.S. Senator\n",
    " d. Governor\n",
    " e. President-Elect (This is a duplicate, we will deal with it in the future)\n",
    " f. U.S. Senator (This is a duplicate, we will deal with it in the future)\n",
    " e. Presidential Candidate </li>\n",
    "<li>\n",
    "4. As per the <b>venue</b> column, meaning the source of the news article, we have:\n",
    " a. 18% of them come from a news release.\n",
    " b. 16.7% of them come from an interview.\n",
    " c. 16.5 come from a press release.\n",
    " d. 15.1% of them come from a speech.\n",
    " e. 13.0% come from a TV ad.\n",
    " f. 11.1% of them come from a tweet.\n",
    " e. 9.6% of them come from a campaign ad.</li>\n",
    "<li>\n",
    "5. As per the <b>state info</b> where the news was released, we have:\n",
    " a. Unknown\n",
    " b. Texas\n",
    " c. Florida\n",
    " d. Wisconsin\n",
    " e. New York\n",
    " f. Illinois\n",
    " g. Ohio</li>\n",
    "<li>\n",
    "6. And at last, for the <b>party affiliation</b> we have:\n",
    " a. 45%.2 of the political parties affiliated to the news articles are republicans.\n",
    " b. We have also a big number of articles affiliated with democrats and 17.$ are unknowns.\n",
    " c. We have also a small number of articles being related to no party affiliations, libertarians, a few of them being independent, affiliated to organizations or newsmakers.\n",
    "</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796d5c99-9d80-4ba4-a28e-00404e7b9efb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">  \n",
    "    <center><h2><strong>Identification of suitable features and implementation of a suitable feature extractor, e.f.\n",
    "TfidfVectorizer</strong></h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7985e084-3d75-4e51-8989-d6d0eecd0307",
   "metadata": {},
   "source": [
    "## 5. Preparing the features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c0de09-ebea-4d8a-a769-160c570e3b77",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b>Below we are going to prepare the features we are going to use with a few techniques in order to prepare the data so that we can further fit our model into it.</b>\n",
    "\n",
    "<ul>\n",
    "<li>1. We will remove the stopwords for the news statements, so to not overfit our model due to these words not being useful for us.</li>\n",
    "<li>2. We will remove special characters.</li>\n",
    "<li>3. We will gather and remove words that are repeated.</li>\n",
    "<li>4. We will transform the string texts in tokens (tokenization) so that we can later vectorize them so to fit our model.</li>\n",
    "<li>5. We will apply stemming to the words in the words so to remove common suffixes from the end of word tokens.</li>\n",
    "</li>6. At last we will apply lemmatization to ensure that the output word is an existing normalized word.</li>\n",
    "</ul>\n",
    "<hr>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de48e2a-7d2f-4d25-a260-3d4097954980",
   "metadata": {},
   "source": [
    "### 5.1 Preparing the statement column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd1b065-9fdc-4cdb-9763-38ef62c3d358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us first define some helper functions that will allow us to prepare the data\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.corpus import stopwords\n",
    "from collections import OrderedDict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "''' \n",
    "Removes stopwords that are included in the english stopwords corpus.\n",
    "\n",
    "Parameters:\n",
    "- column(pd.Dataframe): The dataframe column to be processed \n",
    "\n",
    "Returns:\n",
    "- column(pd.Dataframe): The processed dataframe column\n",
    "'''\n",
    "def remove_stopwords(column):\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    return column.apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords_list)]))\n",
    "\n",
    "\n",
    "''' \n",
    "Removes all the ASCII code special characters !@#$%^&*()_+{}/ from each row in the dataframe column.\n",
    "\n",
    "Parameters:\n",
    "- column(pd.Dataframe): The dataframe column to be processed \n",
    "\n",
    "Returns:\n",
    "- column(pd.Dataframe): The processed dataframe column\n",
    "'''\n",
    "def remove_special_characters(column):\n",
    "    # Using regex to remove every non word character\n",
    "    return column.map(lambda x: re.sub(r'\\W+', ' ', x))\n",
    "\n",
    "\n",
    "''' \n",
    "Removes duplicated words in the dataframe column.\n",
    "\n",
    "Parameters:\n",
    "- column(pd.Dataframe): The dataframe column to be processed \n",
    "\n",
    "Returns:\n",
    "- column(pd.Dataframe): The processed dataframe column\n",
    "'''\n",
    "def remove_repeated_words(column):\n",
    "    ''' \n",
    "    Adds all the words in a string sentence as unique words in a set.\n",
    "    \n",
    "    Parameters:\n",
    "    - text(String): A string value representing the sentence to be processed\n",
    "    \n",
    "    Returns:\n",
    "    - text(String): The processed sentence\n",
    "    '''\n",
    "    def remove_duplicates(text):\n",
    "        words = text.split()\n",
    "        seen = set()\n",
    "        unique_words = []\n",
    "        for word in words:\n",
    "            if word not in seen:\n",
    "                unique_words.append(word)\n",
    "                seen.add(word)\n",
    "        return ' '.join(unique_words)\n",
    "\n",
    "    return column.apply(remove_duplicates)\n",
    "\n",
    "\n",
    "''' \n",
    "Tokenizes the words in the rows belonging to the dataframe column using word_tokenize. \n",
    "\n",
    "Parameters:\n",
    "- column(pd.Dataframe): The dataframe column to be processed\n",
    "\n",
    "Returns:\n",
    "- column(pd.Dataframe): The processed dataframe column\n",
    "'''\n",
    "def tokenize(column):\n",
    "    return column.apply(word_tokenize)\n",
    "\n",
    "\n",
    "''' \n",
    "Applies stemming to the rows belonging to the dataframe column using the Porter Stemmer technique.\n",
    "\n",
    "Parameters:\n",
    "- column(pd.Dataframe): The dataframe column to be processed \n",
    "\n",
    "Returns:\n",
    "- column(pd.Dataframe): The processed dataframe column\n",
    "'''\n",
    "def apply_stemming(column):\n",
    "    stemmer = PorterStemmer()\n",
    "    return column.apply(lambda x : [stemmer.stem(y) for y in x])\n",
    "\n",
    "''' \n",
    "Applies lemmatization to the rows belonging to the dataframe column. using WordNetLemmatizer\n",
    "\n",
    "Parameters:\n",
    "- column(pd.Dataframe): The dataframe column to be processed \n",
    "\n",
    "Returns:\n",
    "- column(pd.Dataframe): The processed dataframe column\n",
    "'''\n",
    "def apply_lemmatization(column):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def lemmatize_text(text):\n",
    "        if isinstance(text, str):\n",
    "            words = word_tokenize(text)\n",
    "            lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "            return ' '.join(lemmatized_words)\n",
    "        else:\n",
    "            return text\n",
    "\n",
    "    return column.apply(lemmatize_text)\n",
    "    \n",
    "# Uncomment the below for debugging\n",
    "# # df[\"statement\"] = remove_stopwords(df[\"statement\"])\n",
    "# # df[\"statement\"] = remove_special_characters(df[\"statement\"])\n",
    "# # df[\"statement\"] = remove_repeated_words(df[\"statement\"])\n",
    "# # df[\"statement\"] = tokenize(df[\"statement\"])\n",
    "# # df[\"statement\"] = apply_stemming(df[\"statement\"])\n",
    "# # df[\"statement\"] = apply_lemmatization(df[\"statement\"])\n",
    "# # df[\"statement\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121b347e-cbd7-47c6-a17d-db37c082428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the statement column\n",
    "\n",
    "# Uncomment the below for debugging\n",
    "# df = pd.read_csv(\"datasets/Liar_Dataset.csv\")\n",
    "\n",
    "df[\"statement\"] = remove_stopwords(df[\"statement\"])\n",
    "df[\"statement\"] = remove_special_characters(df[\"statement\"])\n",
    "df[\"statement\"] = remove_repeated_words(df[\"statement\"])\n",
    "df[\"statement\"] = tokenize(df[\"statement\"])\n",
    "df[\"statement\"] = apply_stemming(df[\"statement\"])\n",
    "df[\"statement\"] = apply_lemmatization(df[\"statement\"])\n",
    "df[\"statement\"].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9161b8-d991-4c12-a509-fdb4f7d0a05f",
   "metadata": {},
   "source": [
    "### 5.2. Preparing the subject(s) column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7fcfa9-2aa9-431b-895e-302ab3b6922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us define some helper functions to allow us to prepare the subject(s) column\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "''' \n",
    "Uses the fuzzywuzy library to group similar words together on a similarity threshold.\n",
    "\n",
    "Parameters:\n",
    "- column(pd.Dataframe): The dataframe column to be processed \n",
    "\n",
    "Returns:\n",
    "- column(pd.Dataframe): The processed dataframe column\n",
    "'''\n",
    "def group_by_similar_lists(column, threshold=80):\n",
    "    unique_values = column.dropna().unique()\n",
    "    groups = {}\n",
    "\n",
    "    # Convert list of strings to a single string for comparison\n",
    "    def list_to_string(lst):\n",
    "        return ''.join(lst)\n",
    "\n",
    "    # Iterate through each unique value (list of strings) and assign it to a group\n",
    "    for value in unique_values:\n",
    "        found_group = False\n",
    "        str_value = list_to_string(value)\n",
    "        \n",
    "        for group in groups:\n",
    "            # If the value is similar to the representative value of the group, add it to the group\n",
    "            if fuzz.ratio(str_value, group) > threshold:\n",
    "                groups[group].append(value)\n",
    "                found_group = True\n",
    "                break\n",
    "        \n",
    "        # If no similar group was found, create a new group\n",
    "        if not found_group:\n",
    "            groups[str_value] = [value]\n",
    "\n",
    "    # Map each original value to its corresponding group\n",
    "    def map_to_group(value):\n",
    "        str_value = list_to_string(value)\n",
    "        for group, group_values in groups.items():\n",
    "            if any(fuzz.ratio(str_value, list_to_string(existing_value)) > threshold for existing_value in group_values):\n",
    "                return group\n",
    "        return str_value\n",
    "\n",
    "    return column.apply(map_to_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cda563-e016-49f0-bb43-7662b318b59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the below for debugging. Do not forget to comment again so to not mess with our data samples!\n",
    "# df = pd.read_csv(\"datasets/Liar_Dataset.csv\")\n",
    "\n",
    "df[\"subject(s)\"] = remove_stopwords(df[\"subject(s)\"])\n",
    "df[\"subject(s)\"] = remove_special_characters(df[\"subject(s)\"])\n",
    "df[\"subject(s)\"] = remove_repeated_words(df[\"subject(s)\"])\n",
    "\n",
    "df[\"subject(s)\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d0f0f8-4589-4ea6-9f83-817a4436b5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By checking the above, we can see that after removing the stopwords, special characters and repeated words from each row in the subject(s) column we are left with\n",
    "# 4533 unique values. Let us try to group similar values so to not overfit our model with unneeded information.\n",
    "\n",
    "df[\"subject(s)\"] = group_by_similar_lists(df[\"subject(s)\"])\n",
    "\n",
    "df[\"subject(s)\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974095ef-78b7-4bb1-afef-728c908bdc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let us plot the top 10 values in our subject(s) list\n",
    "plot_pie(\"subject(s)\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc50b846-83de-4716-8e56-07f3f01c31ec",
   "metadata": {},
   "source": [
    "### 5.3 Preparing the speaker column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b4f69e-e34a-4e33-a08b-5897285b855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us see how many speakers we have. 3308\n",
    "df['speaker'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28a2c23-8fa1-46fa-9014-22176146d886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let us remove the stopwords, remove special characters and repeated words and group by.\n",
    "\n",
    "df['speaker'] = remove_stopwords(df['speaker'])\n",
    "df['speaker'] = remove_special_characters(df['speaker'])\n",
    "df['speaker'] = remove_repeated_words(df['speaker'])\n",
    "df['speaker'] = group_by_similar_lists(df['speaker'])\n",
    "\n",
    "# Now we ended up with 3120 values.\n",
    "df['speaker'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde66b13-204c-4165-965a-3be7f6b3384b",
   "metadata": {},
   "source": [
    "### 5.4 Preparing the speaker's job title column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b64b47-7b65-488a-82dc-91c1b18cb4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us see how many values we have here in our speaker's job title column. We have 1355 values, \n",
    "# let us try to apply our pipeline in order to reduce its dimensionality and remove unneeded words.\n",
    "df[\"speaker's job title\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f594ce-66ae-4a74-bacb-d6aa5bb9cfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for the speaker's job title\n",
    "\n",
    "def tokenize_column(column):\n",
    "    column = column.apply(str)\n",
    "    tokens = []\n",
    "    for word in column:\n",
    "        tokens.append(word_tokenize(word))\n",
    "\n",
    "    return tokens;\n",
    "\n",
    "def remove_stopwords_from_speakers_job_title(speakers_job_title_list):\n",
    "    return [\n",
    "        [word for word in word_list if word.lower() not in stopwords.words('english')]\n",
    "        for word_list in speakers_job_title_list\n",
    "    ]\n",
    "\n",
    "def apply_stemming_to_speakers_job_title(df, column_name, speakers_token):\n",
    "    ps = PorterStemmer() \n",
    "\n",
    "    index = 0    \n",
    "    for words in speakers_token:\n",
    "    \n",
    "        job=\"\"\n",
    "        for w in words: \n",
    "            job=job+ps.stem(w)+\" \"\n",
    "        df.at[index, column_name] = job\n",
    "        index += 1\n",
    "    return df[column_name]\n",
    "\n",
    "\n",
    "# Uncomment below to debug.\n",
    "'''\n",
    "df = pd.read_csv(\"datasets/Liar_Dataset.csv\")\n",
    "\n",
    "speakers_token = tokenize_column(df[\"speaker's job title\"])\n",
    "speakers_token = remove_stopwords_from_speakers_job_title(speakers_token)\n",
    "\n",
    "df[\"speaker's job title\"] = apply_stemming_to_speakers_job_title(df, \"speaker's job title\", speakers_token)\n",
    "df[\"speaker's job title\"] = group_by_similar_lists(df[\"speaker's job title\"])\n",
    "\n",
    "df[\"speaker's job title\"].head(10)\n",
    "df[\"speaker's job title\"].value_counts().head(12).plot(kind='bar')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dfd0c0-f5a0-455a-ba75-12aefcea9986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let us apply the functions above to the column\n",
    "\n",
    "speakers_token = tokenize_column(df[\"speaker's job title\"])\n",
    "speakers_token = remove_stopwords_from_speakers_job_title(speakers_token)\n",
    "\n",
    "df[\"speaker's job title\"] = apply_stemming_to_speakers_job_title(df, \"speaker's job title\", speakers_token)\n",
    "df[\"speaker's job title\"] = group_by_similar_lists(df[\"speaker's job title\"])\n",
    "\n",
    "plot_pie(\"speaker's job title\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3685ced9-a7c5-4361-85a1-e192a3a36bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let us count how many values we have:\n",
    "df[\"speaker's job title\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2478ee10-9e59-43ea-a7fd-38e2c9f427c0",
   "metadata": {},
   "source": [
    "### 5.4 Preparing the venue column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043a5bb0-d0c4-4d77-8ae5-d6f6e81180e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['venue'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40cd1e8-7c3b-4afc-a651-782b1134d14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some helper function for the venue column\n",
    "\n",
    "def remove_stopwords_from_venue(speakers_job_title_list):\n",
    "    return [\n",
    "        [word for word in word_list if word.lower() not in stopwords.words('english')]\n",
    "        for word_list in speakers_job_title_list\n",
    "    ]\n",
    "\n",
    "def apply_stemming_to_venue(df, column_name, tokens):\n",
    "    ps = PorterStemmer() \n",
    "\n",
    "    index = 0    \n",
    "    for words in tokens:\n",
    "    \n",
    "        job=\"\"\n",
    "        for w in words: \n",
    "            job=job+ps.stem(w)+\" \"\n",
    "        df.at[index, column_name] = job\n",
    "        index += 1\n",
    "    return df[column_name]\n",
    "    \n",
    "\n",
    "# Uncomment below to debug.\n",
    "'''\n",
    "df = pd.read_csv(\"datasets/Liar_Dataset.csv\")\n",
    "\n",
    "venues_tokens = tokenize_column(df['venue'])\n",
    "venues_tokens = remove_stopwords_from_venue(venues_tokens)\n",
    "\n",
    "df['venue'] = apply_stemming_to_venue(df, 'venue', venues_tokens)\n",
    "\n",
    "df['venue'].value_counts()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64159617-fdf1-4001-b7b5-753b25fc8bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let us apply tokenization remove the stopwords and apply stemming to the venue column.\n",
    "venues_tokens = tokenize_column(df['venue'])\n",
    "venues_tokens = remove_stopwords_from_venue(venues_tokens)\n",
    "\n",
    "df['venue'] = apply_stemming_to_venue(df, 'venue', venues_tokens)\n",
    "\n",
    "# Now we can see that we have 4591 values left in the column\n",
    "df['venue'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c918446-7cd5-418f-81bf-b5dc0ae51b08",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">  \n",
    "    <center><h2><strong>Feature extraction</strong></h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0a65f9-eef5-44b9-a696-80628750e3bc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b>In this section we are going to convert the raw text data from our features into vectors that we can use later to feed our machine learning models. We are going\n",
    "to do this using the TF-IDF (Term Frequency-Inverse Document Frequency) technique, Adjusts the word counts of each element based on their importance (frequency and inverse-frequency) across the entire dataseta.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53409cc-c148-417f-ac75-e969d17ac747",
   "metadata": {},
   "source": [
    "## 6. Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0bfae4-158c-4cd8-8162-4c85228e2460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let us define some helper functions for the vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Let us instantiate our vectorizer using TF IDF vectorizer\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "        sublinear_tf=True,\n",
    "        strip_accents='unicode',\n",
    "        analyzer='word',\n",
    "        token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(3, 3),\n",
    "        max_features =5000)\n",
    "\n",
    "def vectorize_column(column):\n",
    "    return word_vectorizer.fit_transform(column.astype('str'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d89178d-01f5-4dc3-be0e-5f0594bece03",
   "metadata": {},
   "source": [
    "### 6.1 Feature extraction on the statements column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1594db0-d1dc-44a2-8dff-0f5545ddf61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "statements_vectors = vectorize_column(df['statement'])\n",
    "statements_vectors = statements_vectors.toarray()\n",
    "\n",
    "# Get output feature names for transformation.\n",
    "output_feaature_names = word_vectorizer.get_feature_names_out(df['statement'])\n",
    "\n",
    "statement_feature_df=pd.DataFrame(np.round(statements_vectors, 1), columns=output_feaature_names)\n",
    "statement_feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dae131-7971-4da7-bda3-c333eb09a7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the vectorized statement column to the dataframe\n",
    "df = pd.concat([df, statement_feature_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1494cb41-09ea-48bb-bf30-e23173dd6527",
   "metadata": {},
   "source": [
    "### 6.2 Feature extraction for the Subject's column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68efe98-8c6d-4f05-b018-852f366f0962",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_vectors = vectorize_column(df[\"subject(s)\"])\n",
    "subjects_vectors = subjects_vectors.toarray()\n",
    "\n",
    "# Get output feature names for transformation.\n",
    "output_feature_names_subject = word_vectorizer.get_feature_names_out(df[\"subject(s)\"])\n",
    "\n",
    "subjects_feature_df=pd.DataFrame(np.round(subjects_vectors, 1), columns=output_feature_names_subject)\n",
    "subjects_feature_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddbba60-4ba8-4428-8dd4-d07e4f1fed61",
   "metadata": {},
   "source": [
    "### 6.3 Feature extraction on the speakerÂ´s job title column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcff6f5-4233-40b5-b148-98632f8d0d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers_job_title_vectors = vectorize_column(df[\"speaker's job title\"])\n",
    "speakers_job_title_vectors = speakers_job_title_vectors.toarray()\n",
    "\n",
    "# Get output feature names for transformation.\n",
    "output_feature_names_speakers_job_title = word_vectorizer.get_feature_names_out(df[\"speaker's job title\"])\n",
    "\n",
    "speakers_job_title_feature_df=pd.DataFrame(np.round(speakers_job_title_vectors, 1), columns=output_feature_names_speakers_job_title)\n",
    "speakers_job_title_feature_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0f2abc-0e65-45b4-a715-481014d231ef",
   "metadata": {},
   "source": [
    "### 6.3 Feature extraction on the party affiliation column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144283d0-53c8-41d3-a873-1fa03382d61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "party_affiliation_vectors = vectorize_column(df[\"party affiliation\"])\n",
    "party_affiliation_vectors = party_affiliation_vectors.toarray()\n",
    "\n",
    "# Get output feature names for transformation.\n",
    "output_feature_names_party_affiliation = word_vectorizer.get_feature_names_out(df[\"party affiliation\"])\n",
    "\n",
    "party_affiliation_feature_df=pd.DataFrame(np.round(party_affiliation_vectors, 1), columns=output_feature_names_party_affiliation)\n",
    "party_affiliation_feature_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4ddcee-ed54-489e-a811-d84a76c3e124",
   "metadata": {},
   "source": [
    "### 6.4 Feature extraction on the venue column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad5fa53-ee81-4527-9a8e-8c0094443d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "venue_vectors = vectorize_column(df[\"venue\"])\n",
    "venue_vectors = venue_vectors.toarray()\n",
    "\n",
    "# Get output feature names for transformation.\n",
    "output_feature_names_venue = word_vectorizer.get_feature_names_out(df[\"venue\"])\n",
    "\n",
    "venue_feature_df=pd.DataFrame(np.round(venue_vectors, 1), columns=output_feature_names_venue)\n",
    "venue_feature_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeedd8a4-02b3-4d63-9413-8868d7790dde",
   "metadata": {},
   "source": [
    "### 6.6 Adding the features to the dataframe so we can feed them to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f784ed8a-65b3-49d4-91b8-f087f7a66cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_vectors = pd.Categorical(df['speaker'])               \n",
    "df['speaker']=speaker_vectors.codes\n",
    "\n",
    "label_vectors = pd.Categorical(df['label'])               \n",
    "df['label']=label_vectors.codes\n",
    "\n",
    "state_info_vectors = pd.Categorical(df['state info'])               \n",
    "df['state info']=state_info_vectors.codes\n",
    "\n",
    "subjects_info_vectors = pd.Categorical(df[\"subject(s)\"])               \n",
    "df[\"subject(s)\"]=subjects_info_vectors.codes\n",
    "\n",
    "speakers_job_title_info_vectors = pd.Categorical(df[\"speaker's job title\"])               \n",
    "df[\"speaker's job title\"]=speakers_job_title_info_vectors.codes\n",
    "\n",
    "party_affiliation_info_vectors = pd.Categorical(df[\"party affiliation\"])               \n",
    "df[\"party affiliation\"]=party_affiliation_info_vectors.codes\n",
    "\n",
    "venue_info_vectors = pd.Categorical(df[\"venue\"])               \n",
    "df[\"venue\"]=venue_info_vectors.codes\n",
    "\n",
    "# I am going to drop the statement column because it has too many rows and it is making our models too slow\n",
    "df.drop(columns=['statement'], axis=1, inplace=True)\n",
    "\n",
    "# Let us check the dataframe now with all the features already vectorized\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8526b4-6a5d-429b-9cec-7eaf8b61c533",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">  \n",
    "    <center><h2><strong>Building and evaluation our models</strong></h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89530ba-ad12-4770-a2db-abbe2fecec38",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b>In this section we are going to build and train our machine learning models and use K Fold cross validation for model comparison. Here we chose to use three supervised learning classification models from sklearn, being them: Gaussian Naive Bayes, Neural Networks and Logistic Regression. We are also going to use KFold cross-validation in order to split our test and train data within 5 iterations.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff7d1f3-3804-46c2-afff-9a661797931f",
   "metadata": {},
   "source": [
    "## 6.1 Machine Learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72a3f89-37a4-48e4-a227-152e8291ca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us define some helper variables and functions in order to better evaluate our models\n",
    "\n",
    "# Importing the necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Global variables\n",
    "kfold_splits_5 = 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59666686-6a66-4a42-a1d2-a75556649cfd",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbdd0f2-8d50-4591-91e6-d406eb0b4b54",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b>In this section we are going to build and train our machine learning models and use K Fold cross validation for model comparison.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8e9f4c-c6e0-4488-b587-ba3ded23580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Naive Bayes model\n",
    "nb = GaussianNB(var_smoothing=1e-08)\n",
    "\n",
    "# Split features and labels\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Set up K-Fold cross-validation\n",
    "kf = KFold(n_splits=kfold_splits_5)\n",
    "outcomes = []\n",
    "conf_matrix_list = []\n",
    "\n",
    "# Cross-validation loop\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X, y), start=1):\n",
    "    print(f\"KFold Split: {fold}\")\n",
    "    print(f\"Train indices: {train_index}\")\n",
    "    print(f\"Test indices: {test_index}\\n\")\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Train the model and measure time\n",
    "    print('Running time of algorithm')\n",
    "    start_time = time.time()\n",
    "    nb.fit(X_train, y_train)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time:.4f} seconds\")\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = nb.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    outcomes.append(accuracy)\n",
    "    print(f\"Accuracy of KFold {fold} is: {accuracy}\\n\")\n",
    "    \n",
    "    # Generate and print classification report\n",
    "    print(f\"Classification Report of KFold {fold} is following:\\n\")\n",
    "    classification_rep = classification_report(y_test, predictions)\n",
    "    print(classification_rep)\n",
    "    \n",
    "    # Generate and print confusion matrix\n",
    "    print(f\"Confusion Matrix of KFold {fold} is following:\\n\")\n",
    "    confusion_mat = confusion_matrix(y_test, predictions)\n",
    "    conf_matrix_list.append(confusion_mat)\n",
    "    print(confusion_mat)\n",
    "    print('\\n' + '='*50 + '\\n')\n",
    "\n",
    "# Print the average results\n",
    "mean_accuracy = np.mean(outcomes)\n",
    "print(f\"Total Average Accuracy of Naive Bayes is: {mean_accuracy:.4f}\")\n",
    "print(\"\\nAverage Confusion Matrix:\\n\")\n",
    "avg_conf_matrix = np.mean(conf_matrix_list, axis=0)\n",
    "print(avg_conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f4e2fa-5b8d-49d6-9822-d7adbe96edc5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b>By analysing the report above we conclude the following.</b>\n",
    "<ul>\n",
    "<li>1.The accuracy of its predictions was 0.9984. We will use this as a baseline in order to compare with the other models.</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a475e714-405b-4a90-b0d7-a639a6849a84",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36331a2-de28-4cd5-a9b2-e9144e3a6653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MLPClassifier model\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, alpha=0.0001, solver='adam', random_state=1)\n",
    "\n",
    "# Split features and labels\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Set up K-Fold cross-validation\n",
    "kf = KFold(n_splits=kfold_splits_5)\n",
    "outcomes = []\n",
    "conf_matrix_list = []\n",
    "\n",
    "# Cross-validation loop\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X, y), start=1):\n",
    "    print(f\"KFold Split: {fold}\")\n",
    "    print(f\"Train indices: {train_index}\")\n",
    "    print(f\"Test indices: {test_index}\\n\")\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Train the model and measure time\n",
    "    print('Running time of algorithm')\n",
    "    start_time = time.time()\n",
    "    mlp.fit(X_train, y_train)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time:.4f} seconds\")\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = mlp.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    outcomes.append(accuracy)\n",
    "    print(f\"Accuracy of KFold {fold} is: {accuracy}\\n\")\n",
    "    \n",
    "    # Generate and print classification report\n",
    "    print(f\"Classification Report of KFold {fold} is following:\\n\")\n",
    "    classification_rep = classification_report(y_test, predictions)\n",
    "    print(classification_rep)\n",
    "    \n",
    "    # Generate and print confusion matrix\n",
    "    print(f\"Confusion Matrix of KFold {fold} is following:\\n\")\n",
    "    confusion_mat = confusion_matrix(y_test, predictions)\n",
    "    conf_matrix_list.append(confusion_mat)\n",
    "    print(confusion_mat)\n",
    "    print('\\n' + '='*50 + '\\n')\n",
    "\n",
    "# Print the average results\n",
    "mean_accuracy = np.mean(outcomes)\n",
    "print(f\"Total Average Accuracy of MLP Classifier is: {mean_accuracy:.4f}\")\n",
    "print(\"\\nAverage Confusion Matrix:\\n\")\n",
    "avg_conf_matrix = np.mean(conf_matrix_list, axis=0)\n",
    "print(avg_conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f83829a-025c-4d6a-8c31-155b41124d6b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b>By analysing the report above we conclude the following.</b>\n",
    "<ul>\n",
    "<li>1.The accuracy of its predictions was 0.5789. We will use this as a baseline in order to compare with the other models.</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27b3f98-4d19-485f-8c9a-9a883901be6e",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49f9687-f8ea-461b-b5dc-ece636c9bc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "log_reg = LogisticRegression(solver='lbfgs', max_iter=7600, random_state=1)\n",
    "\n",
    "# Split features and labels\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Set up K-Fold cross-validation\n",
    "kf = KFold(n_splits=kfold_splits_5)\n",
    "outcomes = []\n",
    "conf_matrix_list = []\n",
    "\n",
    "# Cross-validation loop\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X, y), start=1):\n",
    "    print(f\"KFold Split: {fold}\")\n",
    "    print(f\"Train indices: {train_index}\")\n",
    "    print(f\"Test indices: {test_index}\\n\")\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Train the model and measure time\n",
    "    print('Running time of algorithm')\n",
    "    start_time = time.time()\n",
    "    log_reg.fit(X_train, y_train)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time:.4f} seconds\")\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = log_reg.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    outcomes.append(accuracy)\n",
    "    print(f\"Accuracy of KFold {fold} is: {accuracy}\\n\")\n",
    "    \n",
    "    # Generate and print classification report\n",
    "    print(f\"Classification Report of KFold {fold} is following:\\n\")\n",
    "    classification_rep = classification_report(y_test, predictions)\n",
    "    print(classification_rep)\n",
    "    \n",
    "    # Generate and print confusion matrix\n",
    "    print(f\"Confusion Matrix of KFold {fold} is following:\\n\")\n",
    "    confusion_mat = confusion_matrix(y_test, predictions)\n",
    "    conf_matrix_list.append(confusion_mat)\n",
    "    print(confusion_mat)\n",
    "    print('\\n' + '='*50 + '\\n')\n",
    "\n",
    "# Print the average results\n",
    "mean_accuracy = np.mean(outcomes)\n",
    "print(f\"Total Average Accuracy of Logistic Regression is: {mean_accuracy:.4f}\")\n",
    "print(\"\\nAverage Confusion Matrix:\\n\")\n",
    "avg_conf_matrix = np.mean(conf_matrix_list, axis=0)\n",
    "print(avg_conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84dfb23-4bad-423c-af7a-0b68d25ebed0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b>By analysing the report above we conclude the following.</b>\n",
    "<ul>\n",
    "<li>1.The accuracy of its predictions was 0.5789. We will use this as a baseline in order to compare with the other models.</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bf89bf-6f59-4ab3-894f-c12e8d09f2e7",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a2377a-6be9-4567-9c5a-68ab63598c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd0c70b-668b-4c71-a4b1-2ddece8efeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use Wikipedia pip module in order to evaluate the model precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7caa1a7-0d2e-4e96-95ec-89e2b33f24a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia \n",
    " \n",
    "# finding result for the search\n",
    "# sentences = 2 refers to numbers of line\n",
    "result = wikipedia.page(title=\"Elections_in_the_United_States\")\n",
    "# result.content\n",
    " \n",
    "# printing the result\n",
    "print(result.section(\"Criticisms\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49a9351-d098-4008-8c3c-f0f892beb559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Example text to classify\n",
    "random_text = result.section(\"Criticisms\")\n",
    "\n",
    "# Step 2: Preprocess and vectorize the input text\n",
    "text_vector = vectorizer.transform([random_text])\n",
    "\n",
    "# Step 3: Make the prediction\n",
    "prediction = log_reg.predict(text_vector)\n",
    "\n",
    "# Optionally, get the probability of the prediction\n",
    "prediction_proba = log_reg.predict_proba(text_vector)\n",
    "\n",
    "# Output the result\n",
    "print(f\"Predicted class: {prediction[0]}\")\n",
    "print(f\"Prediction probabilities: {prediction_proba[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ece8d9-7d45-4185-a7ce-f66b54ddee2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
