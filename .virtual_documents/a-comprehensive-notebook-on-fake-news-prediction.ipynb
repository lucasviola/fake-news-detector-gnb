

















import numpy as np

import warnings
warnings.filterwarnings("ignore")





df = pd.read_csv("/kaggle/input/liar-twitter-dataset/Liar_Dataset.csv") 











df.head(5)





df.tail(5)





df.drop(['[ID].json'], axis=1, inplace=True)





df.columns





print('lenght of data is', len(df))





df.shape





df.info()





df.dtypes





np.sum(df.isnull().any(axis=1))





print('Count of columns in the data is:  ', len(df.columns))


print('Count of rows in the data is:  ', len(df))





current=len(df)
print('Rows of data before Delecting ', current)


df=df.drop_duplicates()


now=len(df)
print('Rows of data before Delecting ', now)


diff=current-now
print('Duplicated rows are ', diff)





df.isnull().sum()





df.replace('', np.nan, inplace=True)





df['venue']= df['venue'].replace(np.nan, 'Unknown')
df["speaker's job title"]= df["speaker's job title"].replace(np.nan, 'Unknown')
df["state info"]= df["state info"].replace(np.nan, 'Unknown')





df.isnull().sum()





num_cols = ['barely true counts', 'false counts', 'half true counts', 'mostly true counts', 'pants on fire counts']
num_cols





cate_cols = df.columns.drop('label').drop(num_cols)
cate_cols





df[cate_cols].apply(lambda x: x.nunique(), axis=0)














sns.countplot(data= df, x = "label")
plt.show()


df["label"].value_counts().head(7).plot(kind = 'pie', autopct='%1.1f%%', figsize=(8, 8)).legend(bbox_to_anchor=(1, 1))


df["label"].value_counts()





data1=df[df['label']=='barely-true']
d =data1['statement']
string_ = []
for t in d:
    string_.append(t)
string_ = pd.Series(string_).map(str)
string_=str(string_)
wordcloud = WordCloud(width=1500, height=700,max_font_size=250, background_color ='white').generate(string_)
plt.figure(figsize=(12,10))
plt.imshow(wordcloud)
plt.axis("off")
plt.show()





data1=df[df['label']=='half-true']
d =data1['statement']
string_ = []
for t in d:
    string_.append(t)
string_ = pd.Series(string_).map(str)
string_=str(string_)
wordcloud = WordCloud(width=1500, height=700,max_font_size=250, background_color ='black').generate(string_)
plt.figure(figsize=(12,10))
plt.imshow(wordcloud)
plt.axis("off")
plt.show()





data1=df[df['label']=='mostly-true']
d =data1['statement']
string_ = []
for t in d:
    string_.append(t)
string_ = pd.Series(string_).map(str)
string_=str(string_)
wordcloud = WordCloud(width=1500, height=700,max_font_size=250, background_color ='yellow').generate(string_)
plt.figure(figsize=(12,10))
plt.imshow(wordcloud)
plt.axis("off")
plt.show()





data1=df[df['label']=='TRUE']
d =data1['statement']
string_ = []
for t in d:
    string_.append(t)
string_ = pd.Series(string_).map(str)
string_=str(string_)
wordcloud = WordCloud(width=1500, height=700,max_font_size=250, background_color ='purple').generate(string_)
plt.figure(figsize=(12,10))
plt.imshow(wordcloud)
plt.axis("off")
plt.show()





data1=df[df['label']=='FALSE']
d =data1['statement']
string_ = []
for t in d:
    string_.append(t)
string_ = pd.Series(string_).map(str)
string_=str(string_)
wordcloud = WordCloud(width=1500, height=700,max_font_size=250, background_color ='pink').generate(string_)
plt.figure(figsize=(12,10))
plt.imshow(wordcloud)
plt.axis("off")
plt.show()





data1=df[df['label']=='pants-fire']
d =data1['statement']
string_ = []
for t in d:
    string_.append(t)
string_ = pd.Series(string_).map(str)
string_=str(string_)
wordcloud = WordCloud(width=1500, height=700,max_font_size=250, background_color ='navy').generate(string_)
plt.figure(figsize=(12,10))
plt.imshow(wordcloud)
plt.axis("off")
plt.show()











df['statement']=df['statement'].str.lower()


df['statement'].tail()





stopwords_list = stopwords.words('english')


from nltk.corpus import stopwords
", ".join(stopwords.words('english'))





STOPWORDS = set(stopwords.words('english'))
def cleaning_stopwords(text):
    return " ".join([word for word in str(text).split() if word not in STOPWORDS])
df["statement"] = df["statement"].apply(lambda text: cleaning_stopwords(text))
df["statement"].head()





import string
english_punctuations = string.punctuation
punctuations_list = english_punctuations
def cleaning_punctuations(text):
    translator = str.maketrans('', '', punctuations_list)
    return text.translate(translator)


df["statement"] = df["statement"].apply(lambda x: cleaning_punctuations(x))
df["statement"].tail()





def cleaning_repeating_char(text):
    return re.sub(r'(.)\1+', r'\1', text)


df["statement"] = df["statement"].apply(lambda x: cleaning_repeating_char(x))
df["statement"].tail()





def cleaning_email(data):
    return re.sub('@[^\s]+', ' ', data)


df["statement"] = df["statement"].apply(lambda x: cleaning_email(x))
df["statement"].tail()





def cleaning_URLs(data):
    return re.sub('((www\.[^\s]+)|(https?://[^\s]+))',' ',data)


df["statement"] = df["statement"].apply(lambda x: cleaning_URLs(x))
df["statement"].tail()





def cleaning_numbers(data):
    return re.sub('[0-9]+', '', data)


df["statement"] = df["statement"].apply(lambda x: cleaning_numbers(x))
df["statement"].tail()





tokenizer = RegexpTokenizer(r'\w+')
df["statement"] = df["statement"].apply(tokenizer.tokenize)


df["statement"].head()





st = nltk.PorterStemmer()
def stemming_on_text(data):
    text = [st.stem(word) for word in data]
    return data

df["statement"] = df["statement"].apply(lambda x: stemming_on_text(x))


df["statement"].head()





lm = nltk.WordNetLemmatizer()
def lemmatizer_on_text(data):
    text = [lm.lemmatize(word) for word in data]
    return data

df["statement"] = df["statement"].apply(lambda x: lemmatizer_on_text(x))


df["statement"].head()





words = [word for tokens in df["statement"] for word in tokens]
lenght_of_each_sentence = [len(tokens) for tokens in df["statement"]]
vocabulary  = sorted(list(set(words)))
print("There are %s words in total, with vocabulary size of %s" % (len(words), len(vocabulary)))





counts_of_words = Counter(words)


counts_of_words.most_common(25)


words = []
counts = []
for letter, count in counts_of_words.most_common(25):
    words.append(letter)
    counts.append(count)
colors = cm.rainbow(np.linspace(0, 2, 12))
rcParams['figure.figsize'] = 20, 10
plt.title('Top 25 frequently words in news statement text')
plt.xlabel('Count')
plt.ylabel('Words')
plt.barh(words, counts, color=colors)








df["subject(s)"].head()





df["subject(s)"]= df["subject(s)"].str.replace(",", " ")
subjectTokenize = []
for sen in df["subject(s)"]:
    subjectTokenize.append(word_tokenize(sen))





filteredsubjects = []
for words in subjectTokenize:
    stopWords = set(stopwords.words('english'))
    wordsFiltered = []
    for w in words:
        if w not in stopWords:
            wordsFiltered.append(w)
    filteredsubjects.append(wordsFiltered)






ps = PorterStemmer() 
index = 0    
for words in filteredsubjects:
    subjects=""
    for w in words: 
        subjects=subjects+ps.stem(w)+" "
    df.at[index, "subject(s)"] = subjects
    index += 1





df["dummp"]=df["subject(s)"]
df["dummp"]=df["dummp"].str.strip()
df.loc[df["dummp"].str.contains('job|worker'), 'dummp'] = 'jobs'
df.loc[df["dummp"].str.contains('hous'), 'dummp'] = 'budget'
df.loc[df["dummp"].str.contains('county-budget'), 'dummp'] = 'budget'
df.loc[df["dummp"].str.contains('federal-budget'), 'dummp'] = 'budget'
df.loc[df["dummp"].str.contains('state-budget|city-budget'), 'dummp'] = 'budget'
df.loc[df["dummp"].str.contains('state-fin'), 'dummp'] = 'budget'
df.loc[df["dummp"].str.contains('edu'), 'dummp'] = 'education'
df.loc[df["dummp"].str.contains('economi|incom|tax|debt|market-regul|financial-regul|trade|small-busi'), 'dummp'] = 'economy'
df.loc[df["dummp"].str.contains('militari|veteran'), 'dummp'] = 'military'
df.loc[df["dummp"].str.contains('government-effici|city-govern|county-govern|government-regul|supreme-court|state'), 'dummp'] = 'government'

df.loc[df["dummp"].str.contains('health-car|medicar|abort|public-health'), 'dummp'] = 'health-care'
df.loc[df["dummp"].str.contains('crime|gun|public-safeti|legal-issu|terror|homeland-secur'), 'dummp'] = 'crime'

df.loc[df["dummp"].str.contains('climate-chang|environ|anim'), 'dummp'] = 'environment'

df.loc[df["dummp"].str.contains('foreign-polici|voting-record|congress|elect|politics'), 'dummp'] = 'politics'

df.loc[df["dummp"].str.contains('children|immigr|women|popul|poverti|social-secur|religion'), 'dummp'] = 'social'



df.loc[~df["dummp"].str.contains('jobs|budget|education|economy|military|government|health-care|crime|environment|politics|social'), 'dummp'] = 'other'


df["dummp"].value_counts()


df["subject(s)"]=df["dummp"]


df["subject(s)"].value_counts()


df=df.drop(columns=['dummp'])


jt=[]
jt=df["subject(s)"].unique()
for i in jt:
    print(i)





df["subject(s)"].value_counts().head(12).plot(kind='bar')


df["subject(s)"].value_counts().head(12).plot(kind = 'pie', autopct='%1.1f%%', figsize=(8, 8)).legend(bbox_to_anchor=(1, 1))








df["speaker"].head()





df["speaker"].value_counts().head(10).plot(kind = 'pie', autopct='%1.1f%%', figsize=(8, 8)).legend(bbox_to_anchor=(1, 1))





df["speaker"].value_counts().tail(10).plot(kind = 'pie', autopct='%1.1f%%', figsize=(8, 8)).legend(bbox_to_anchor=(1, 1))





df["speaker"].value_counts().head(50)








df["speaker's job title"] .head()





df["speaker's job title"] = df["speaker's job title"].apply(str)
jobTokenize = []
for sen in df["speaker's job title"]:
    jobTokenize.append(word_tokenize(sen))





from nltk.corpus import stopwords
filteredjobs = []
for words in jobTokenize:
    stopWords = set(stopwords.words('english'))
    wordsFiltered = []
    for w in words:
        if w not in stopWords:
            wordsFiltered.append(w)
    filteredjobs.append(wordsFiltered)





from nltk.stem import PorterStemmer 
ps = PorterStemmer() 

index = 0    
for words in filteredjobs:

    job=""
    for w in words: 
        job=job+ps.stem(w)+" "
    df.at[index, "speaker's job title"] = job
    index += 1





df["dummp"]=df["speaker's job title"]
df["dummp"]=df["dummp"].str.strip()
df.loc[df["dummp"].str.contains('repres'), 'dummp'] = 'U.S. representative'
df.loc[df["dummp"].str.contains('governor'), 'dummp'] = 'state representative'
df.loc[df["dummp"].str.contains('state'), 'dummp'] = 'state representative'
df.loc[df["dummp"].str.contains('congressman'), 'dummp'] = 'state representative'
df.loc[df["dummp"].str.contains('senat'), 'speaker'] = 'state representative'
df.loc[df["dummp"].str.contains('congresswoman'), 'dummp'] = 'state representative'
df.loc[df["dummp"].str.contains('deleg'), 'dummp'] = 'state representative'
df.loc[df["dummp"].str.contains('mayor'), 'dummp'] = 'state representative'
df.loc[df["dummp"].str.contains('presid'), 'dummp'] = 'president'
df.loc[df["dummp"].str.contains('director'), 'dummp'] = 'office director'
df.loc[df["dummp"].str.contains('group'), 'dummp'] = 'company'
df.loc[df["dummp"].str.contains('chairman'), 'dummp'] = 'company'
df.loc[df["dummp"].str.contains('program'), 'dummp'] = 'company'
df.loc[df["dummp"].str.contains('counti'), 'dummp'] = 'government'
df.loc[df["dummp"].str.contains('attorney'), 'dummp'] = 'government'
df.loc[df["dummp"].str.contains('govern'), 'dummp'] = 'government'
df.loc[df["dummp"].str.contains('media'), 'dummp'] = 'media'
df.loc[df["dummp"].str.contains('blog'), 'dummp'] = 'media'
df.loc[df["dummp"].str.contains('show'), 'dummp'] = 'media'
df.loc[df["dummp"].str.contains('host'), 'dummp'] = 'media'
df.loc[df["dummp"].str.contains('radio'), 'dummp'] = 'media'
df.loc[df["dummp"].str.contains('tv'), 'dummp'] = 'media'
df.loc[df["dummp"].str.contains('unknown'), 'dummp'] = 'unknown'


df.loc[~df["dummp"].str.contains('state representative|president|office director|company|U.S. representative|government|media|unknown'), 'dummp'] = 'other'


df["dummp"].value_counts()


df["speaker's job title"]=df["dummp"]


df["speaker's job title"].value_counts()


df=df.drop(columns=['dummp'])


jt=[]
jt=df["speaker's job title"].unique()
for i in jt:
    print(i)





df["speaker's job title"].value_counts().head(12).plot(kind='bar')


df["speaker's job title"].value_counts().head(12).plot(kind = 'pie', autopct='%1.1f%%', figsize=(8, 8)).legend(bbox_to_anchor=(1, 1))








df["state info"].head()





df["state info"].value_counts().head(10).plot(kind = 'pie', autopct='%1.1f%%', figsize=(8, 8)).legend(bbox_to_anchor=(1, 1))





df["state info"].value_counts().tail(10).plot(kind = 'pie', autopct='%1.1f%%', figsize=(8, 8)).legend(bbox_to_anchor=(1, 1))





df["state info"].value_counts().head(50)








df['party affiliation'].head()





party=[]
party=df['party affiliation'].unique()
for i in party:
    print(i)





df["party affiliation"]= df["party affiliation"].replace('none', 'Unknown')
df["party affiliation"]= df["party affiliation"].replace('activist', 'Other')
df["party affiliation"]= df["party affiliation"].replace('organization', 'Other')
df["party affiliation"]= df["party affiliation"].replace('libertarian', 'Other')
df["party affiliation"]= df["party affiliation"].replace('journalist', 'Other')
df["party affiliation"]= df["party affiliation"].replace('columnist', 'Other')
df["party affiliation"]= df["party affiliation"].replace('state-official', 'Other')
df["party affiliation"]= df["party affiliation"].replace('business-leader', 'Other')
df["party affiliation"]= df["party affiliation"].replace('talk-show-host', 'Other')
df["party affiliation"]= df["party affiliation"].replace('government-body', 'Other')
df["party affiliation"]= df["party affiliation"].replace('newsmaker', 'Other')
df["party affiliation"]= df["party affiliation"].replace('county-commissioner', 'Other')
df["party affiliation"]= df["party affiliation"].replace('constitution-party', 'Other')
df["party affiliation"]= df["party affiliation"].replace('labor-leader', 'Other')
df["party affiliation"]= df["party affiliation"].replace('education-official', 'Other')
df["party affiliation"]= df["party affiliation"].replace('tea-party-member', 'Other')
df["party affiliation"]= df["party affiliation"].replace('green', 'Other')
df["party affiliation"]= df["party affiliation"].replace('liberal-party-canada', 'Other')
df["party affiliation"]= df["party affiliation"].replace('Moderate', 'Other')
df["party affiliation"]= df["party affiliation"].replace('democratic-farmer-labor', 'Other')
df["party affiliation"]= df["party affiliation"].replace('ocean-state-tea-party-action', 'Other')
df["party affiliation"]= df["party affiliation"].replace('independent', 'Other')


party=[]
party=df['party affiliation'].unique()
for i in party:
    print(i)





def check_dist(dataset):
  sns.countplot(x='party affiliation', data=df, palette='hls')


check_dist(df)


df["party affiliation"].value_counts().head(12).plot(kind = 'pie', autopct='%1.1f%%', figsize=(8, 8)).legend()








df["venue"].head()





df["venue"] = df["venue"].apply(str)





vanueTokenize = []
for sen_tex in df['venue']:
    vanueTokenize.append(word_tokenize(sen_tex))





filteredVenues = []
for words in vanueTokenize:
    stopWords = set(stopwords.words('english'))
    wordsFiltered = []
    for w in words:
        if w not in stopWords:
            wordsFiltered.append(w)
    filteredVenues.append(wordsFiltered)





ps = PorterStemmer()
index = 0    
for words in filteredVenues:
    vn=""
    for w in words: 
        vn=vn+ps.stem(w)+" "
    df.at[index, 'venue'] = vn
    index += 1





df["dummp"]=df["venue"]
df["dummp"]=df["dummp"].str.strip()
df.loc[df["dummp"].str.contains('confer|press|speech|interview|debate|broadcast|meet|opinion|statement|letter|ralli'), 'dummp'] = 'interview'
df.loc[df["dummp"].str.contains('campaign|ad|flier|commerci|mailer|panel|billboard'), 'dummp'] = 'ad'
df.loc[df["dummp"].str.contains('facebook|imag|media|meme|tweet|email|e-email|forum|blog|twitter'), 'dummp'] = 'social media'

df.loc[df["dummp"].str.contains('abc|articl|news|cnn|msnbc|book|journal|hbo|fox|column|newslett'), 'dummp'] = 'news'
df.loc[df["dummp"].str.contains('websit|web'), 'dummp'] = 'website'

df.loc[df["dummp"].str.contains('show'), 'dummp'] = 'show'

df.loc[df["dummp"].str.contains('unknown'), 'dummp'] = 'unknown'


df.loc[~df["dummp"].str.contains('interview|ad|social media|news|website|show|unknown'), 'dummp'] = 'other'


df["dummp"].value_counts()


df["venue"]=df["dummp"]


df["venue"].value_counts()


df=df.drop(columns=['dummp'])


v=[]
v=df['venue'].unique()
for i in v:
    print(i)





def check_dist(dataset):
    sns.countplot(x='venue', data=df, palette='hls')
check_dist(df)


df["venue"].value_counts().head(12).plot(kind = 'pie', autopct='%1.1f%%', figsize=(8, 8)).legend()











num=df[['barely true counts', 'false counts',
       'half true counts', 'mostly true counts', 'pants on fire counts']]
num=num.fillna(0)
num.head()





num.hist(figsize=(15,12),bins = 20, color="#107009AA")
plt.title("Features Distribution")
plt.show()





word_vectorizer = TfidfVectorizer(
    sublinear_tf=True,
    strip_accents='unicode',
    analyzer='word',
    token_pattern=r'\w{1,}',
    ngram_range=(3, 3),
    max_features =5000)

Get_Vec= word_vectorizer.fit_transform(df['statement'].astype('str'))
Get_Vec= Get_Vec.toarray()

vocab1 = word_vectorizer.get_feature_names()
Features_vect=pd.DataFrame(np.round(Get_Vec, 1), columns=vocab1)
Features_vect.head()





df=df.drop(columns=['statement'])





cleanup_nums = {"venue":     {'interview': 0, 'ad': 1, 'social media': 2,'news': 3, 'website': 4, 'show': 5, 'unknown': -1 , 'other' : 6},
                "speaker's job title": {'state representative': 0, 'president': 1, 'office director': 2, 'company': 3,
                                  'U.S. representative': 4, 'government': 5, 'media':6, 'unknown': -1, 'other' : 7 },
                "party affiliation":     {'republican': 0, 'democrat': 1, 'Unknown': -1,'Other': 2},
                "subject(s)": {'jobs':0, 'military':1, 'education':2, 'economy':3, 'government':4, 'health-care':5, 
                               'crime':6, 'environment':7, 'budget':8, 'politics':9,'social':10, 'other':11}
              }





df.replace(cleanup_nums, inplace=True)
df.head()





df = pd.concat([df, Features_vect], axis=1)





x = pd.Categorical(df['speaker'])               
df['speaker']=x.codes
x = pd.Categorical(df['label'])               
df['label']=x.codes
x = pd.Categorical(df['state info'])               
df['state info']=x.codes


df.head()





df=df[df['label']!=-1]
y = df.label
sns.countplot(data= df, x = y)
plt.show()








kf = KFold(n_splits=5)
i=0
for train, test in kf.split(df):
    i=i+1
    print("KFold Split ",i )
    print("%s %s" % (train, test))
    print(' \n')














rf = RandomForestClassifier(min_samples_leaf=1, min_samples_split=2)
X = df.iloc[:, 0:-1].values
y = df.label
kf = KFold(n_splits=5)
outcomes1 = []
ClassR=0
ConM=0
fold = 0
i=0
conf_matrix_list_of_arrays = []
for train_index, test_index in kf.split(X,y):
    i=i+1
    print("KFold Split:",i)
    print("%s %s" % (train_index, test_index))
    print('\n')
    fold += 1
    Xtrain, Xtest = X[train_index], X[test_index]
    ytrain, ytest = y[train_index], y[test_index]
    print('Running time of algorithm')
    %time rf.fit(Xtrain, ytrain)
    predictions = rf.predict(Xtest)
    accuracy = accuracy_score(ytest, predictions)
    outcomes1.append(accuracy)
    print("Accuracy of KFold ",i, "is: ",accuracy)
    print('\n')
    print("Classification Report of KFold ",i," is following:")
    print('\n')
    CR=classification_report(ytest, predictions)
    print(CR)
    print('\n')
    print("Confusion Matrix of KFold ",i," is following:")
    print('\n')
    CM=confusion_matrix(ytest, predictions)
    conf_matrix_list_of_arrays.append(CM)
    print(CM)
    print('\n')
    print('\n')

print('\n')
print('Average Confusion Matrix')
aa = np.mean(conf_matrix_list_of_arrays, axis=0)

aaa = np.ceil(aa)

b=pd.DataFrame(aaa)
b=b.astype(int)
labels =['False','True','Barely-True','Half-True','Mostly-True','Pans-Fire']

c=np.array(b)

fig, ax = plot_confusion_matrix(conf_mat=c,figsize=(10, 10),
                                show_absolute=True,
                                show_normed=True,
                                colorbar=True)
ax.set_xticklabels([''] + labels)
ax.set_yticklabels([''] + labels)
plt.show()
print('\n')
print('\n')
mean_outcome1 = np.mean(outcomes1)
print("Total Average Accuracy of Random Forest Classifier is : {0}".format(mean_outcome1)) 





nb = GaussianNB(var_smoothing=1e-08)
X = df.iloc[:, 0:-1].values
y = df.label
kf = KFold(n_splits=5)
outcomes2 = []
ClassR=0
ConM=0
fold = 0
i=0
conf_matrix_list_of_arrays = []
for train_index, test_index in kf.split(X,y):
    i=i+1
    print("KFold Split:",i)
    print("%s %s" % (train_index, test_index))
    print('\n')
    fold += 1
    Xtrain, Xtest = X[train_index], X[test_index]
    ytrain, ytest = y[train_index], y[test_index]
    print('Running time of algorithm')
    %time nb.fit(Xtrain, ytrain)
    predictions = nb.predict(Xtest)
    accuracy = accuracy_score(ytest, predictions)
    outcomes2.append(accuracy)
    print("Accuracy of KFold ",i, "is: ",accuracy)
    print('\n')
    print("Classification Report of KFold ",i," is following:")
    print('\n')
    CR=classification_report(ytest, predictions)
    print(CR)
    print('\n')
    print("Confusion Matrix of KFold ",i," is following:")
    print('\n')
    CM=confusion_matrix(ytest, predictions)
    conf_matrix_list_of_arrays.append(CM)
    print(CM)
    print('\n')
    print('\n')
print('\n')
print('Average Confusion Matrix')
aa = np.mean(conf_matrix_list_of_arrays, axis=0)

aaa = np.ceil(aa)

b=pd.DataFrame(aaa)
b=b.astype(int)
labels =['False','True','Barely-True','Half-True','Mostly-True','Pans-Fire']

c=np.array(b)

fig, ax = plot_confusion_matrix(conf_mat=c,figsize=(10, 10),
                                show_absolute=True,
                                show_normed=True,
                                colorbar=True)
ax.set_xticklabels([''] + labels)
ax.set_yticklabels([''] + labels)
plt.show()
print('\n')
print('\n')
mean_outcome2 = np.mean(outcomes2)
print("Total Average Accuracy of Naive bayes is : {0}".format(mean_outcome2)) 





nn = MLPClassifier(activation='relu', alpha=0.0001, batch_size=10,hidden_layer_sizes=(90,), random_state=100,learning_rate=0.01)
X = df.iloc[:, 0:-1].values
y = df.label
kf = KFold(n_splits=5)
outcomes3 = []
ClassR=0
ConM=0
fold = 0
i=0
conf_matrix_list_of_arrays = []
for train_index, test_index in kf.split(X,y):
    i=i+1
    print("KFold Split:",i)
    print("%s %s" % (train_index, test_index))
    print('\n')
    fold += 1
    Xtrain, Xtest = X[train_index], X[test_index]
    ytrain, ytest = y[train_index], y[test_index]
    print('Running time of algorithm')
    %time nn.fit(Xtrain, ytrain)
    predictions = nn.predict(Xtest)
    accuracy = accuracy_score(ytest, predictions)
    outcomes3.append(accuracy)
    print("Accuracy of KFold ",i, "is: ",accuracy)
    print('\n')
    print("Classification Report of KFold ",i," is following:")
    print('\n')
    CR=classification_report(ytest, predictions)
    print(CR)
    print('\n')
    print("Confusion Matrix of KFold ",i," is following:")
    print('\n')
    CM=confusion_matrix(ytest, predictions)
    conf_matrix_list_of_arrays.append(CM)
    print(CM)
    print('\n')
    print('\n')
print('\n')
print('Average Confusion Matrix')
aa = np.mean(conf_matrix_list_of_arrays, axis=0)

aaa = np.ceil(aa)

b=pd.DataFrame(aaa)
b=b.astype(int)
labels =['False','True','Barely-True','Half-True','Mostly-True','Pans-Fire']

c=np.array(b)

fig, ax = plot_confusion_matrix(conf_mat=c,figsize=(10, 10),
                                show_absolute=True,
                                show_normed=True,
                                colorbar=True)
ax.set_xticklabels([''] + labels)
ax.set_yticklabels([''] + labels)
plt.show()
print('\n')
print('\n')
mean_outcome3 = np.mean(outcomes3)
print("Total Average Accuracy of Neural Network is : {0}".format(mean_outcome3)) 





dt = DecisionTreeClassifier(min_samples_leaf=900)
X = df.iloc[:, 0:-1].values
y = df.label
kf = KFold(n_splits=5)
outcomes4 = []
ClassR=0
ConM=0
fold = 0
i=0
conf_matrix_list_of_arrays = []
for train_index, test_index in kf.split(X,y):
    i=i+1
    print("KFold Split:",i)
    print("%s %s" % (train_index, test_index))
    print('\n')
    fold += 1
    Xtrain, Xtest = X[train_index], X[test_index]
    ytrain, ytest = y[train_index], y[test_index]
    print('Running time of algorithm')
    %time nn.fit(Xtrain, ytrain)
    predictions = nn.predict(Xtest)
    accuracy = accuracy_score(ytest, predictions)
    outcomes4.append(accuracy)
    print("Accuracy of KFold ",i, "is: ",accuracy)
    print('\n')
    print("Classification Report of KFold ",i," is following:")
    print('\n')
    CR=classification_report(ytest, predictions)
    print(CR)
    print('\n')
    print("Confusion Matrix of KFold ",i," is following:")
    print('\n')
    CM=confusion_matrix(ytest, predictions)
    conf_matrix_list_of_arrays.append(CM)
    print(CM)
    print('\n')
    print('\n')
print('\n')
print('Average Confusion Matrix')
aa = np.mean(conf_matrix_list_of_arrays, axis=0)

aaa = np.ceil(aa)

b=pd.DataFrame(aaa)
b=b.astype(int)
labels =['False','True','Barely-True','Half-True','Mostly-True','Pans-Fire']

c=np.array(b)

fig, ax = plot_confusion_matrix(conf_mat=c,figsize=(10, 10),
                                show_absolute=True,
                                show_normed=True,
                                colorbar=True)
ax.set_xticklabels([''] + labels)
ax.set_yticklabels([''] + labels)
plt.show()
print('\n')
print('\n')
mean_outcome4 = np.mean(outcomes4)
print("Total Average Accuracy of Decision Trees is : {0}".format(mean_outcome4)) 





a=pd.DataFrame()
a['outcomes1']=outcomes1
a['outcomes2']=outcomes2
a['outcomes3']=outcomes3
a['outcomes4']=outcomes4

plt.figure(figsize=(25, 10))
plt.subplot(1,1,1)
plt.plot(a.outcomes1.values,color='blue',label='Random Forest')
plt.plot(a.outcomes2.values,color='green',label='Naive Bayes')
plt.plot(a.outcomes3.values,color='yellow',label='Neural Network')
plt.plot(a.outcomes4.values,color='red',label='Decision Trees')
plt.title('Algorithms Comparison')
plt.xlabel('Number of time')
plt.ylabel('Accuracy')
plt.legend(bbox_to_anchor=(1, 1))
plt.show()


a['outcomes1']=outcomes1
a['outcomes2']=outcomes2
a['outcomes3']=outcomes3
a['outcomes4']=outcomes4

plt.figure(figsize=(25, 10))
plt.subplot(1,1,1)
plt.plot(a.outcomes1.values,color='blue',label='Random Forest')
plt.plot(a.outcomes2.values,color='green',label='Naive Bayes')
plt.plot(a.outcomes3.values,color='yellow',label='Neural Network')
plt.plot(a.outcomes4.values,color='red',label='Decision Trees')
plt.title('Algorithms Comparison')
plt.xlabel('Number of time')
plt.ylabel('Accuracy')
plt.legend(loc=True)
plt.show()


a=a.rename(columns={'outcomes1':'Random Forest', 'outcomes2':'Naive Bayes','outcomes3':'Neural Netwrok','outcomes4':'Decision Trees'})


a.plot(kind='bar',figsize=(25, 10))





a








x = PrettyTable()
print('\n')
print("Best Model.")
x.field_names = ["Model", "Accuracy"]
x.add_row(["Naive Bayes Algorithm",a['Naive Bayes']])
print(x)
print('\n')





















































a















