








#Importing the required libraries

import numpy as np
import pandas as pd





# Here we are loading th dataset into a pandas dataframe we can do further analysis on it
df = pd.read_csv("datasets/Liar_Dataset.csv")

# We are also going to look at the first 5 records in it so to have an idead on what we are working with
df.head(5)





# Let us first take a look at what we are dealing with
df.info()





# Now take a look at the columns inside the dataset so to have an idea about the features we can use further in our model
df.columns








# We already noticed above that we have some null values. Now let us confirm how many and in which columns
df.isnull().sum()


np.sum(df.isnull().any(axis=1))





# TODO: Maybe change the strategy
# Here we are re replacing the rows with NaN (null) values with a blank space
df.replace('', np.nan, inplace=True)

# Now we are going to replace the np.nan values for all the three columns we are cleaning
df["speaker's job title"]= df["speaker's job title"].replace(np.nan, 'Unknown')
df['venue']= df['venue'].replace(np.nan, 'Unknown')
df["state info"]= df["state info"].replace(np.nan, 'Unknown')


# Now let us check again if we were able to completely remove the nulls
df.isnull().sum()


# TODO: Fix
# We can also remove th [ID].json column, since it is not useful to us
# df.drop(columns=['[ID].json'], axis=1, inplace=True)








# TODO: Fix helper functions and add more visualizations

# Defining some helper functions and variables for generating the visualizations using matplotlib, wordcloud and seaborn
from matplotlib import pyplot as plt 
from wordcloud import WordCloud
import seaborn as sns

number_of_elements_to_display = 10


# Plots a pie graph based on a specific column
def plot_pie(column, number_of_values):
    df[column].value_counts().head(number_of_values).plot(kind = 'pie', autopct='%1.1f%%', figsize=(8, 8)).legend(bbox_to_anchor=(1, 1))

def plot_bar(column, number_of_values):
    df[column].value_counts().value_counts().head(number_of_values).plot(x='lab', y='val', rot=0)

# TODO: Fix because we need a way to show the data correlation between different columns.
# Plots a wordcloud based on the relationship between two columns
## column: 
## value:
## word:
# def plot_wordcloud(column, value, word):
#     data1=df[df[column]==value]
#     d =data1[word]
#     string_ = []
#     for t in d:
#         string_.append(t)
#     string_ = pd.Series(string_).map(str)
#     string_=str(string_)
#     wc = WordCloud(width=1500, height=700,max_font_size=250, background_color ='white').generate(string_)
#     plt.figure(figsize=(12,10))
#     plt.imshow(wc)
#     plt.axis("off")
#     plt.show()

# TODO: Not working because too many words
# def plot_distribution_graph(column):
#     sns.countplot(data= df, x = column)
#     plt.show()

# plot_distribution_graph('subject(s)')


# Plotting a pie graph based on the label column
plot_pie('label', number_of_elements_to_display)


# Plotting a pie graph based on the subjects column
plot_pie('subject(s)', number_of_elements_to_display)


# Plotting a pie graph based on the subjects speakers
plot_pie('speaker', number_of_elements_to_display)


# Plotting a pie graph based on the speaker's job title column
plot_pie("speaker's job title", number_of_elements_to_display)


# Plotting a pie graph based on the venue
plot_pie("venue", number_of_elements_to_display)


# Plotting a pie graph based on the speaker's job title column
plot_pie("state info", number_of_elements_to_display)


# Plotting a pie graph based on the speaker's job title column
plot_pie("party affiliation", number_of_elements_to_display)





# TODO: Correlate the columns above with the categories of news with wordclouds or distribution graphs.

















# Let us first define some helper functions that will allow us to prepare the data
import re
from nltk.corpus import stopwords
from collections import OrderedDict
from nltk.tokenize import word_tokenize
from nltk import RegexpTokenizer
from nltk.stem.porter import *
from nltk.stem import WordNetLemmatizer

''' 
Removes stopwords that are included in the english stopwords corpus.

Parameters:
- column(pd.Dataframe): The dataframe column to be processed 

Returns:
- column(pd.Dataframe): The processed dataframe column
'''
def remove_stopwords(column):
    stopwords_list = stopwords.words('english')
    return column.apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords_list)]))


''' 
Removes all the ASCII code special characters !@#$%^&*()_+{}/ from each row in the dataframe column.

Parameters:
- column(pd.Dataframe): The dataframe column to be processed 

Returns:
- column(pd.Dataframe): The processed dataframe column
'''
def remove_special_characters(column):
    # Using regex to remove every non word character
    return column.map(lambda x: re.sub(r'\W+', ' ', x))


''' 
Removes duplicated words in the dataframe column.

Parameters:
- column(pd.Dataframe): The dataframe column to be processed 

Returns:
- column(pd.Dataframe): The processed dataframe column
'''
def remove_repeated_words(column):
    ''' 
    Adds all the words in a string sentence as unique words in a set.
    
    Parameters:
    - text(String): A string value representing the sentence to be processed
    
    Returns:
    - text(String): The processed sentence
    '''
    def remove_duplicates(text):
        words = text.split()
        seen = set()
        unique_words = []
        for word in words:
            if word not in seen:
                unique_words.append(word)
                seen.add(word)
        return ' '.join(unique_words)

    return column.apply(remove_duplicates)


''' 
Tokenizes the words in the rows belonging to the dataframe column using word_tokenize. 

Parameters:
- column(pd.Dataframe): The dataframe column to be processed

Returns:
- column(pd.Dataframe): The processed dataframe column
'''
def tokenize(column):
    return column.apply(word_tokenize)


''' 
Applies stemming to the rows belonging to the dataframe column using the Porter Stemmer technique.

Parameters:
- column(pd.Dataframe): The dataframe column to be processed 

Returns:
- column(pd.Dataframe): The processed dataframe column
'''
def apply_stemming(column):
    stemmer = PorterStemmer()
    return column.apply(lambda x : [stemmer.stem(y) for y in x])

''' 
Applies lemmatization to the rows belonging to the dataframe column. using WordNetLemmatizer

Parameters:
- column(pd.Dataframe): The dataframe column to be processed 

Returns:
- column(pd.Dataframe): The processed dataframe column
'''
def apply_lemmatization(column):
    lemmatizer = WordNetLemmatizer()

    def lemmatize_text(text):
        if isinstance(text, str):
            words = word_tokenize(text)
            lemmatized_words = [lemmatizer.lemmatize(word) for word in words]
            return ' '.join(lemmatized_words)
        else:
            return text

    return column.apply(lemmatize_text)
    
# Uncomment the below for debugging
# # df["statement"] = remove_stopwords(df["statement"])
# # df["statement"] = remove_special_characters(df["statement"])
# # df["statement"] = remove_repeated_words(df["statement"])
# # df["statement"] = tokenize(df["statement"])
# # df["statement"] = apply_stemming(df["statement"])
# # df["statement"] = apply_lemmatization(df["statement"])
# # df["statement"].head()


# Preparing the statement column

# Uncomment the below for debugging
# df = pd.read_csv("datasets/Liar_Dataset.csv")

df["statement"] = remove_stopwords(df["statement"])
df["statement"] = remove_special_characters(df["statement"])
df["statement"] = remove_repeated_words(df["statement"])
df["statement"] = tokenize(df["statement"])
df["statement"] = apply_stemming(df["statement"])
df["statement"] = apply_lemmatization(df["statement"])
df["statement"].head(5)





# Let us define some helper functions to allow us to prepare the subject(s) column
from fuzzywuzzy import fuzz
from fuzzywuzzy import process

import pandas as pd
from fuzzywuzzy import fuzz

''' 
Uses the fuzzywuzy library to group similar words together on a similarity threshold.

Parameters:
- column(pd.Dataframe): The dataframe column to be processed 

Returns:
- column(pd.Dataframe): The processed dataframe column
'''
def group_by_similar_lists(column, threshold=80):
    unique_values = column.dropna().unique()
    groups = {}

    # Convert list of strings to a single string for comparison
    def list_to_string(lst):
        return ''.join(lst)

    # Iterate through each unique value (list of strings) and assign it to a group
    for value in unique_values:
        found_group = False
        str_value = list_to_string(value)
        
        for group in groups:
            # If the value is similar to the representative value of the group, add it to the group
            if fuzz.ratio(str_value, group) > threshold:
                groups[group].append(value)
                found_group = True
                break
        
        # If no similar group was found, create a new group
        if not found_group:
            groups[str_value] = [value]

    # Map each original value to its corresponding group
    def map_to_group(value):
        str_value = list_to_string(value)
        for group, group_values in groups.items():
            if any(fuzz.ratio(str_value, list_to_string(existing_value)) > threshold for existing_value in group_values):
                return group
        return str_value

    return column.apply(map_to_group)


# Uncomment the below for debugging. Do not forget to comment again so to not mess with our data samples!
# df = pd.read_csv("datasets/Liar_Dataset.csv")

df["subject(s)"] = remove_stopwords(df["subject(s)"])
df["subject(s)"] = remove_special_characters(df["subject(s)"])
df["subject(s)"] = remove_repeated_words(df["subject(s)"])
# df["subject(s)"] = tokenize(df["subject(s)"])

df["subject(s)"].value_counts()


# By checking the above, we can see that after removing the stopwords, special characters and repeated words from each row in the subject(s) column we are left with
# 4533 unique values. Let us try to group similar values so to not overfit our model with unneeded information.

df["subject(s)"] = group_by_similar_lists(df["subject(s)"])

df["subject(s)"].value_counts()


# now let us plot the top 10 values in our subject(s) list
plot_pie("subject(s)", 10)





# Let us see how many speakers we have. 3308
df['speaker'].value_counts()


# Now let us remove the stopwords, remove special characters and repeated words and group by.

df['speaker'] = remove_stopwords(df['speaker'])
df['speaker'] = remove_special_characters(df['speaker'])
df['speaker'] = remove_repeated_words(df['speaker'])
df['speaker'] = group_by_similar_lists(df['speaker'])

# Now we ended up with 3120 values.
df['speaker'].value_counts()





# Let us see how many values we have here in our speaker's job title column. We have 1355 values, 
# let us try to apply our pipeline in order to reduce its dimensionality and remove unneeded words.
df["speaker's job title"].value_counts()


# Helper functions for the speaker's job title

def tokenize_column(column):
    column = column.apply(str)
    tokens = []
    for word in column:
        tokens.append(word_tokenize(word))

    return tokens;

def remove_stopwords_from_speakers_job_title(speakers_job_title_list):
    return [
        [word for word in word_list if word.lower() not in stopwords.words('english')]
        for word_list in speakers_job_title_list
    ]

def apply_stemming_to_speakers_job_title(df, column_name, speakers_token):
    ps = PorterStemmer() 

    index = 0    
    for words in speakers_token:
    
        job=""
        for w in words: 
            job=job+ps.stem(w)+" "
        df.at[index, column_name] = job
        index += 1
    return df[column_name]


# Uncomment below to debug.
'''
df = pd.read_csv("datasets/Liar_Dataset.csv")

speakers_token = tokenize_column(df["speaker's job title"])
speakers_token = remove_stopwords_from_speakers_job_title(speakers_token)

df["speaker's job title"] = apply_stemming_to_speakers_job_title(df, "speaker's job title", speakers_token)
df["speaker's job title"] = group_by_similar_lists(df["speaker's job title"])

df["speaker's job title"].head(10)
df["speaker's job title"].value_counts().head(12).plot(kind='bar')
'''


# Now let us apply the functions above to the column

speakers_token = tokenize_column(df["speaker's job title"])
speakers_token = remove_stopwords_from_speakers_job_title(speakers_token)

df["speaker's job title"] = apply_stemming_to_speakers_job_title(df, "speaker's job title", speakers_token)
df["speaker's job title"] = group_by_similar_lists(df["speaker's job title"])

plot_pie("speaker's job title", 10)


# Now let us count how many values we have:
df["speaker's job title"].value_counts()





df['venue'].value_counts()


# Defining some helper function for the venue column

def remove_stopwords_from_venue(speakers_job_title_list):
    return [
        [word for word in word_list if word.lower() not in stopwords.words('english')]
        for word_list in speakers_job_title_list
    ]

def apply_stemming_to_venue(df, column_name, tokens):
    ps = PorterStemmer() 

    index = 0    
    for words in tokens:
    
        job=""
        for w in words: 
            job=job+ps.stem(w)+" "
        df.at[index, column_name] = job
        index += 1
    return df[column_name]
    

# Uncomment below to debug.
'''
df = pd.read_csv("datasets/Liar_Dataset.csv")

venues_tokens = tokenize_column(df['venue'])
venues_tokens = remove_stopwords_from_venue(venues_tokens)

df['venue'] = apply_stemming_to_venue(df, 'venue', venues_tokens)

df['venue'].value_counts()
'''


# Now let us apply tokenization remove the stopwords and apply stemming to the venue column.
venues_tokens = tokenize_column(df['venue'])
venues_tokens = remove_stopwords_from_venue(venues_tokens)

df['venue'] = apply_stemming_to_venue(df, 'venue', venues_tokens)

# Now we can see that we have 4591 values left in the column
df['venue'].value_counts()














# First let us define some helper functions for the vectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

# Let us instantiate our vectorizer using TF IDF vectorizer
word_vectorizer = TfidfVectorizer(
        sublinear_tf=True,
        strip_accents='unicode',
        analyzer='word',
        token_pattern=r'\w{1,}',
        ngram_range=(3, 3),
        max_features =5000)

def vectorize_column(column):
    return word_vectorizer.fit_transform(column.astype('str'))





statements_vectors = vectorize_column(df['statement'])
statements_vectors = statements_vectors.toarray()

# Get output feature names for transformation.
output_feaature_names = word_vectorizer.get_feature_names_out(df['statement'])

statement_feature_df=pd.DataFrame(np.round(statements_vectors, 1), columns=output_feaature_names)
statement_feature_df.head()


# Adding the vectorized statement column to the dataframe
df = pd.concat([df, statement_feature_df], axis=1)





# TODO: FIX
# subjects_vectors = vectorize_column(df["subject(s)"])
# # subjects_vectors = statements_vectors.toarray()

# # Get output feature names for transformation.
# output_feature_names2 = word_vectorizer.get_feature_names_out(df["subject(s)"])

# statement_feature_df=pd.DataFrame(np.round(subjects_vectors, 1), columns=output_feature_names2)
# statement_feature_df.head()





speaker_vectors = pd.Categorical(df['speaker'])               
df['speaker']=speaker_vectors.codes

label_vectors = pd.Categorical(df['label'])               
df['label']=label_vectors.codes

state_info_vectors = pd.Categorical(df['state info'])               
df['state info']=state_info_vectors.codes

df.head()








# First let us define some helper function for the K Fold
from sklearn.model_selection import KFold

# Since we have a large dataset we are going to split our training and test data into 10 folds and run our algorithm for 10 iterations.
kf = KFold(n_splits=10)
i=0
for train, test in kf.split(df):
    i=i+1
    print("KFold Split ",i )
    print("%s %s" % (train, test))
    print(' \n')











nb = GaussianNB(var_smoothing=1e-08)
X = df.iloc[:, 0:-1].values
y = df.label
kf = KFold(n_splits=5)
outcomes2 = []
ClassR=0
ConM=0
fold = 0
i=0
conf_matrix_list_of_arrays = []
for train_index, test_index in kf.split(X,y):
    i=i+1
    print("KFold Split:",i)
    print("%s %s" % (train_index, test_index))
    print('\n')
    fold += 1
    Xtrain, Xtest = X[train_index], X[test_index]
    ytrain, ytest = y[train_index], y[test_index]
    print('Running time of algorithm')
    %time nb.fit(Xtrain, ytrain)
    predictions = nb.predict(Xtest)
    accuracy = accuracy_score(ytest, predictions)
    outcomes2.append(accuracy)
    print("Accuracy of KFold ",i, "is: ",accuracy)
    print('\n')
    print("Classification Report of KFold ",i," is following:")
    print('\n')
    CR=classification_report(ytest, predictions)
    print(CR)
    print('\n')
    print("Confusion Matrix of KFold ",i," is following:")
    print('\n')
    CM=confusion_matrix(ytest, predictions)
    conf_matrix_list_of_arrays.append(CM)
    print(CM)
    print('\n')
    print('\n')
print('\n')
print('Average Confusion Matrix')
aa = np.mean(conf_matrix_list_of_arrays, axis=0)

aaa = np.ceil(aa)

b=pd.DataFrame(aaa)
b=b.astype(int)
labels =['False','True','Barely-True','Half-True','Mostly-True','Pans-Fire']

c=np.array(b)

fig, ax = plot_confusion_matrix(conf_mat=c,figsize=(10, 10),
                                show_absolute=True,
                                show_normed=True,
                                colorbar=True)
ax.set_xticklabels([''] + labels)
ax.set_yticklabels([''] + labels)
plt.show()
print('\n')
print('\n')
mean_outcome2 = np.mean(outcomes2)
print("Total Average Accuracy of Naive bayes is : {0}".format(mean_outcome2)) 








# TODO: Use Wikipedia pip module in order to evaluate the model precision
