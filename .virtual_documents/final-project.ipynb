








#Importing the required libraries

import numpy as np
import pandas as pd





# Here we are loading th dataset into a pandas dataframe we can do further analysis on it
df = pd.read_csv("datasets/Liar_Dataset.csv")

# We are also going to look at the first 5 records in it so to have an idead on what we are working with
df.head(5)





# Let us first take a look at what we are dealing with
df.info()





# Now take a look at the columns inside the dataset so to have an idea about the features we can use further in our model
df.columns








# We already noticed above that we have some null values. Now let us confirm how many and in which columns
df.isnull().sum()


np.sum(df.isnull().any(axis=1))





# TODO: Maybe change the strategy
# Here we are re replacing the rows with NaN (null) values with a blank space
df.replace('', np.nan, inplace=True)

# Now we are going to replace the np.nan values for all the three columns we are cleaning
df["speaker's job title"]= df["speaker's job title"].replace(np.nan, 'Unknown')
df['venue']= df['venue'].replace(np.nan, 'Unknown')
df["state info"]= df["state info"].replace(np.nan, 'Unknown')


# Now let us check again if we were able to completely remove the nulls
df.isnull().sum()


# TODO: Fix
# We can also remove th [ID].json column, since it is not useful to us
# df.drop(columns=['[ID].json'], axis=1, inplace=True)








# TODO: Fix helper functions and add more visualizations

# Defining some helper functions for generating the visualizations using matplotlib, wordcloud and seaborn
from matplotlib import pyplot as plt 
from wordcloud import WordCloud
import seaborn as sns

# Plots a pie graph based on a specific column
def plot_pie(column):
    df[column].value_counts().head(7).plot(kind = 'pie', autopct='%1.1f%%', figsize=(8, 8)).legend(bbox_to_anchor=(1, 1))

# TODO: Fix because we need a way to show the data correlation between different columns.
# Plots a wordcloud based on the relationship between two columns
## column: 
## value:
## word:
# def plot_wordcloud(column, value, word):
#     data1=df[df[column]==value]
#     d =data1[word]
#     string_ = []
#     for t in d:
#         string_.append(t)
#     string_ = pd.Series(string_).map(str)
#     string_=str(string_)
#     wc = WordCloud(width=1500, height=700,max_font_size=250, background_color ='white').generate(string_)
#     plt.figure(figsize=(12,10))
#     plt.imshow(wc)
#     plt.axis("off")
#     plt.show()

# TODO: Not working because too many words
# def plot_distribution_graph(column):
#     sns.countplot(data= df, x = column)
#     plt.show()

# plot_distribution_graph('subject(s)')


# Plotting a pie graph based on the label column
plot_pie('label')


# Plotting a pie graph based on the subjects column
plot_pie('subject(s)')


# Plotting a pie graph based on the speaker's job title column
plot_pie("speaker's job title")


# Plotting a pie graph based on the venue
plot_pie("venue")


# Plotting a pie graph based on the speaker's job title column
plot_pie("state info")


# Plotting a pie graph based on the speaker's job title column
plot_pie("party affiliation")





# TODO: Correlate the columns above with the categories of news with wordclouds or distribution graphs.








# Let us first define some helper functions that will allow us to prepare the data
from nltk.corpus import stopwords
import string

# Uncomment the below for debugging
df = pd.read_csv("datasets/Liar_Dataset.csv")

# Removes stopwords that are included in the english stopwords corpus
def remove_stopwords(column):
    stopwords_list = stopwords.words('english')
    return column.apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords_list)]))

def remove_special_characters(column):
    punctuations_list = string.punctuation

    translator = str.maketrans('', '', punctuations_list)
    return column.translate(translator)



# Uncomment the below for debugging
df["statement"] = remove_stopwords(df["statement"].head(5))
df["statement"]




















# TODO: Use Wikipedia pip module in order to evaluate the model precision
