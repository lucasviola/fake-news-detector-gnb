








#Importing the required libraries

import numpy as np
import pandas as pd





# Here we are loading th dataset into a pandas dataframe we can do further analysis on it
df = pd.read_csv("datasets/Liar_Dataset.csv")

# We are also going to look at the first 5 records in it so to have an idead on what we are working with
df.head(5)





# Let us first take a look at what we are dealing with
df.info()





# Now take a look at the columns inside the dataset so to have an idea about the features we can use further in our model
df.columns








# We already noticed above that we have some null values. Now let us confirm how many and in which columns
df.isnull().sum()


np.sum(df.isnull().any(axis=1))





# TODO: Maybe change the strategy
# Here we are re replacing the rows with NaN (null) values with a blank space
df.replace('', np.nan, inplace=True)

# Now we are going to replace the np.nan values for all the three columns we are cleaning
df["speaker's job title"]= df["speaker's job title"].replace(np.nan, 'Unknown')
df['venue']= df['venue'].replace(np.nan, 'Unknown')
df["state info"]= df["state info"].replace(np.nan, 'Unknown')


# Now let us check again if we were able to completely remove the nulls
df.isnull().sum()


# TODO: Fix
# We can also remove th [ID].json column, since it is not useful to us
# df.drop(columns=['[ID].json'], axis=1, inplace=True)








# TODO: Fix helper functions and add more visualizations

# Defining some helper functions for generating the visualizations using matplotlib, wordcloud and seaborn
from matplotlib import pyplot as plt 
from wordcloud import WordCloud
import seaborn as sns

# Plots a pie graph based on a specific column
def plot_pie(column, number_of_values):
    df[column].value_counts().head(number_of_values).plot(kind = 'pie', autopct='%1.1f%%', figsize=(8, 8)).legend(bbox_to_anchor=(1, 1))

# TODO: Fix because we need a way to show the data correlation between different columns.
# Plots a wordcloud based on the relationship between two columns
## column: 
## value:
## word:
# def plot_wordcloud(column, value, word):
#     data1=df[df[column]==value]
#     d =data1[word]
#     string_ = []
#     for t in d:
#         string_.append(t)
#     string_ = pd.Series(string_).map(str)
#     string_=str(string_)
#     wc = WordCloud(width=1500, height=700,max_font_size=250, background_color ='white').generate(string_)
#     plt.figure(figsize=(12,10))
#     plt.imshow(wc)
#     plt.axis("off")
#     plt.show()

# TODO: Not working because too many words
# def plot_distribution_graph(column):
#     sns.countplot(data= df, x = column)
#     plt.show()

# plot_distribution_graph('subject(s)')


# Plotting a pie graph based on the label column
plot_pie('label', 7)


# Plotting a pie graph based on the subjects column
plot_pie('subject(s)', 7)


# Plotting a pie graph based on the speaker's job title column
plot_pie("speaker's job title", 7)


# Plotting a pie graph based on the venue
plot_pie("venue", 7)


# Plotting a pie graph based on the speaker's job title column
plot_pie("state info", 7)


# Plotting a pie graph based on the speaker's job title column
plot_pie("party affiliation", 7)





# TODO: Correlate the columns above with the categories of news with wordclouds or distribution graphs.











# Let us first define some helper functions that will allow us to prepare the data
import re
from nltk.corpus import stopwords
from collections import OrderedDict
from nltk.tokenize import word_tokenize
from nltk import RegexpTokenizer
from nltk.stem.porter import *
from nltk.stem import WordNetLemmatizer

''' 
Removes stopwords that are included in the english stopwords corpus.

Parameters:
- column(pd.Dataframe): The dataframe column to be processed 

Returns:
- column(pd.Dataframe): The processed dataframe column
'''
def remove_stopwords(column):
    stopwords_list = stopwords.words('english')
    return column.apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords_list)]))


''' 
Removes all the ASCII code special characters !@#$%^&*()_+{}/ from each row in the dataframe column.

Parameters:
- column(pd.Dataframe): The dataframe column to be processed 

Returns:
- column(pd.Dataframe): The processed dataframe column
'''
def remove_special_characters(column):
    # Using regex to remove every non word character
    return column.map(lambda x: re.sub(r'\W+', ' ', x))


''' 
Removes duplicated words in the dataframe column.

Parameters:
- column(pd.Dataframe): The dataframe column to be processed 

Returns:
- column(pd.Dataframe): The processed dataframe column
'''
def remove_repeated_words(column):
    ''' 
    Adds all the words in a string sentence as unique words in a set.
    
    Parameters:
    - text(String): A string value representing the sentence to be processed
    
    Returns:
    - text(String): The processed sentence
    '''
    def remove_duplicates(text):
        words = text.split()
        seen = set()
        unique_words = []
        for word in words:
            if word not in seen:
                unique_words.append(word)
                seen.add(word)
        return ' '.join(unique_words)

    return column.apply(remove_duplicates)


''' 
Tokenizes the words in the rows belonging to the dataframe column using word_tokenize. 

Parameters:
- column(pd.Dataframe): The dataframe column to be processed

Returns:
- column(pd.Dataframe): The processed dataframe column
'''
def tokenize(column):
    return column.apply(word_tokenize)


''' 
Applies stemming to the rows belonging to the dataframe column using the Porter Stemmer technique.

Parameters:
- column(pd.Dataframe): The dataframe column to be processed 

Returns:
- column(pd.Dataframe): The processed dataframe column
'''
def apply_stemming(column):
    stemmer = PorterStemmer()
    return column.apply(lambda x : [stemmer.stem(y) for y in x])

''' 
Applies lemmatization to the rows belonging to the dataframe column. using WordNetLemmatizer

Parameters:
- column(pd.Dataframe): The dataframe column to be processed 

Returns:
- column(pd.Dataframe): The processed dataframe column
'''
def apply_lemmatization(column):
    lemmatizer = WordNetLemmatizer()

    def lemmatize_text(text):
        if isinstance(text, str):
            words = word_tokenize(text)
            lemmatized_words = [lemmatizer.lemmatize(word) for word in words]
            return ' '.join(lemmatized_words)
        else:
            return text

    return column.apply(lemmatize_text)
    
# Uncomment the below for debugging
# # df["statement"] = remove_stopwords(df["statement"])
# # df["statement"] = remove_special_characters(df["statement"])
# # df["statement"] = remove_repeated_words(df["statement"])
# # df["statement"] = tokenize(df["statement"])
# # df["statement"] = apply_stemming(df["statement"])
# # df["statement"] = apply_lemmatization(df["statement"])
# # df["statement"].head()


# Preparing the statement column

# Uncomment the below for debugging
# df = pd.read_csv("datasets/Liar_Dataset.csv")

df["statement"] = remove_stopwords(df["statement"])
df["statement"] = remove_special_characters(df["statement"])
df["statement"] = remove_repeated_words(df["statement"])
df["statement"] = tokenize(df["statement"])
df["statement"] = apply_stemming(df["statement"])
df["statement"] = apply_lemmatization(df["statement"])
df["statement"].head(5)





# Let us define some helper functions to allow us to prepare the subject(s) column
from fuzzywuzzy import fuzz
from fuzzywuzzy import process

import pandas as pd
from fuzzywuzzy import fuzz

def group_by_similar_lists(df, column_name, threshold=80):
    unique_values = df[column_name].dropna().unique()
    groups = {}

    # Convert list of strings to a single string for comparison
    def list_to_string(lst):
        return ''.join(lst)

    # Iterate through each unique value (list of strings) and assign it to a group
    for value in unique_values:
        found_group = False
        str_value = list_to_string(value)
        
        for group in groups:
            # If the value is similar to the representative value of the group, add it to the group
            if fuzz.ratio(str_value, group) > threshold:
                groups[group].append(value)
                found_group = True
                break
        
        # If no similar group was found, create a new group
        if not found_group:
            groups[str_value] = [value]

    # Map each original value to its corresponding group
    def map_to_group(value):
        str_value = list_to_string(value)
        for group, group_values in groups.items():
            if any(fuzz.ratio(str_value, list_to_string(existing_value)) > threshold for existing_value in group_values):
                return group
        return str_value

    return df[column_name].apply(map_to_group)





# By checking the above, we can see that after removing the stopwords, special characters and repeated words from each row in the subject(s) column we are left with
# 4533 unique values. Let us try to group similar values so to not overfit our model with unneeded information.

# Uncomment the below for debugging
# df = pd.read_csv("datasets/Liar_Dataset.csv")

df["subject(s)"] = remove_stopwords(df["subject(s)"])
df["subject(s)"] = remove_special_characters(df["subject(s)"])
df["subject(s)"] = remove_repeated_words(df["subject(s)"])
# df["subject(s)"] = tokenize(df["subject(s)"])
# df["subject(s)"] = apply_stemming(df["subject(s)"])

df["subject(s)"].value_counts()

df["subject(s)"] = group_by_similar_lists(df, "subject(s)")

df["subject(s)"].value_counts()


# now let us plot the top 10 values in our subject(s) list
plot_pie("subject(s)", 10)











# TODO: Use Wikipedia pip module in order to evaluate the model precision
